Difference between Gulp and my task runner???
  - Gulp is geared towards running a tree of tasks, test-openapi towards a flat list in parallel
  - Gulp always bail on first error, test-openapi not always
  - declarative vs imperative
  - reporting:
      - Gulp reporting is geared towards success (verbose on success, with a simple error on failure) test-openapi towards failure/validation (silent on success, with a rich error on failure)
      - test-openapi has rich standardized error properties
      - test-openapi can use several reporters
  - test-openapi is more geared towards task return value:
      - Gulp is more verbose (and less idiomatic) to do it
      - test-openapi allows for plugin to separate return value for next plugins to one for other tasks
      - test-openapi allows each plugin in the chain to return a value
  - resource is request/response instead of array of files
  - chain of tasks (GULP.parallel|series): could create plugin
  - watch (GULP.watch()): could create plugin
  - order is managed by plugins (weight) instead of by consumers (source code order)
  - initialization is done slightly differently (start handler instead of constructor)
  - Gulp has no concept of plugin end/cleanup

Plugin order:
  - base it on other things like pluginConfig.dependencies, config.plugins array order or task.* keys order???
  - goals:
      - making it easier for plugin writers
      - making order more obvious for users

DepReqs???
  - depReq value does not get normalization from `start` handlers, i.e. it creates issues (e.g. changing task.call.* or task.validate.* does not create task.call.params nor task.validate.schemas)
  - should use a marker around them to distinguish from other normal strings
      - use same markers as variables???

Tasks results caching
  - maybe config|task.cached BOOL???

variables:
  - use {{ }} inside string values or keys
       - can be escaped as {{{ }}}
       - whitespaces trimmed inside
       - other marker???
  - can be:
       - env.ENV_VAR
       - env.VAR from config.env.*
       - task.* from current task
            - as modified by previous plugins
            - i.e. order dependent???
       - otherTask.*: depReqs
  - crawled and substituted before each plugin 'task' handler
     - in any case, should be after YAML parsing, to make sure it remains valid YAML|JSON
  - allow JSON reference to function taking the same as above as parameters
  - in task files but not in spec
  - validation:
      - markers might make validation hard, e.g. validating that it is valid JSON schema
      - should it be substituted as early as possible???
      - what about depReqs???
  - when loading env vars, should try parse their values as JSON (since env vars values can only be strings), then default to leave as is (no quotes)
  - merge to $data feature???

Aliases???
  - e.g. conf.env.* values can use markers themselves

How to fix 'invalid'???
  - code is still there, unused
  - remember that param.isRandom false|true is now param.random undefined|STR

OperationId should be unlikely to start with it. Or prefix another way??? Think of depReqs

Use Express-style path parameters markers instead of OpenAPI-style???

Bail behavior???
  - should use Bail out! in TAP output

HTTP requests:
  - optimal number of sockets/connections:
     - figure it out
     - enforce it
  - using Node.js http core module instead of fetch library??? Check if:
      - would bring extra features
      - would improve performance
      - would not be too complex

Caching / only re-running failed requests???

Naming:
  - autoclient, autorequest, autotask
  - autoprover, autovalidate

Allow sending MongoDB requests instead of HTTP, for populating task???

load/performance testing???
  - a separate 'time' plugin could attach timing info to task.time.*
  - then validate against task.load.*
  - could validate:
     - max NUM repeated requests fail (def: 0)
        - for any reason: during 'call' plugin, 'validate' plugin (e.g. 500), etc.
     - max time
     - average/median time
  - should report nicely

Running server:
  - should pass an environment variable to help feature flags
      - not NODE_ENV because it is Node.js specific
  - possibilities:
      - npm task
      - Node.js file
      - binary file: programming language agnostic
  - what about closing???
      - signal, e.g. SIGINT or SIGTERM
          - programming language agnostic
      - export a closing function
  - what about server ready event???
  - what about server output/log???

Test coverage:
  - can wrap server start with istanbul
     - then pass options down
  - problem: node.js specific

Standard HTTP headers:
  - own repository
  - JSON schemas v4 for every standard or common HTTP header
  - write in YAML, compile to JSON
  - also have thin wrappers to validate (with ajv) and generate (with json-schema-faker)
  - each header should also include following properties:
     - spec OBJ: name STR (e.g. "RFC 1980"), url STR
  - JSON schemas need $data to other headers, request|response body and status code???
     - how to make it work with json-schema-faker???
     - circular references???
  - use in autovalidate:
      - merged to each task.request|response.headers
         - less priority than task.* (including [task.]each.*) but more priority than specification
     - decide on few required ones on generation (e.g. User-Agent [C]) and validation (e.g. Date [S])

Follow redirects??? Both from 3** and response headers redirects.

Add fuzzy testing values???
  - maybe propose as an option to JSON-SCHEMA-FAKER???
